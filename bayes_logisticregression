'''
Created on 7 Nov, 2016

@author: luotianyi
'''
import numpy
from numpy import *
from sklearn.feature_extraction.text import TfidfTransformer
import math
from scipy.optimize import minimize

def logistic_prob(X, w):
    trunc = 8.  # exp(8)/(1+exp(8)) = 0.9997 which is close enough to 1 as to not matter in most cases.
    z = numpy.dot(X, w)
    z = numpy.clip(z, -trunc, trunc)
    pr = numpy.exp(z)
    pr = pr / (1. + pr)

    return pr

def cal_sigmoid(train_tfidf_array_index_X, initial_theta):
    linear_multiply_value = 0.0
    sigmoid_value = 0.0
    for index_X in range(len(train_tfidf_array_index_X)):
        linear_multiply_value += train_tfidf_array_index_X[index_X] * initial_theta[index_X]
    sigmoid_value = 1.0 / (1.0 + math.exp(-1.0*linear_multiply_value))
    return sigmoid_value

def cost(w, w_prior, H, y_temp, X_temp):
    X = numpy.array(X_temp,dtype=float)
    y = numpy.array(y_temp,dtype=float)
    mu = logistic_prob(X, w)
    #temp_x = numpy.dot(y.T, numpy.log(mu))
    #print temp_x
    cost_value = (-(numpy.dot(y.T, numpy.log(mu)) + numpy.dot((1.0 - y).T, numpy.log(1.0 - mu)))
                        + 0.5 * numpy.dot((w - w_prior).T, numpy.dot(H, (w - w_prior))))
    return cost_value
    
def gradient(w, w_prior, H, y_temp, X_temp):
    X = numpy.array(X_temp,dtype=float)
    y = numpy.array(y_temp,dtype=float)
    mu = logistic_prob(X, w)
    gradient_value = numpy.dot(X.T, (mu - y)) + numpy.dot(H, (w - w_prior))
    return gradient_value
    
def hessian(w, w_prior, H, y, X):    
    mu = logistic_prob(X, w)
    S = mu * (1.0 - mu)
    hessian_value = numpy.dot(X.T, X * S[:, numpy.newaxis]) + H
    return hessian_value
    
def fit_bayes_logistic(y, X, w_prior, H, max_iterartions = 10000):
    # Do the regression
    results_optimize = minimize(cost, w_prior, args=(w_prior, H, y, X), jac=gradient, hess=hessian, method='Newton-CG', options={'maxiter': max_iterartions})
    w_fit = results_optimize.x
    H_fit = hessian(w_fit, w_prior, H, y, X)

    return w_fit, H_fit

def bayes_logistic_prob(X, w, H):

    # set a truncation exponent
    trunc = 8.  # exp(8)/(1+exp(8)) = 0.9997 which is close enough to 1 as to not matter in most cases.

    # unmoderated argument of exponent
    z_a = numpy.dot(X, w)

    H_inv_ = numpy.linalg.inv(H)
    sig2_a = numpy.sum(X * numpy.dot(H_inv_, X.T).T, axis=1)

    # get the moderation factor. Implicit in here is approximating the logistic sigmoid with
    # a probit by setting the probit and sigmoid slopes to be equal at the origin. This is where
    # the factor of pi/8 comes from.
    kappa_sig2_a = 1. / numpy.sqrt(1. + 0.125 * numpy.pi * sig2_a)

    # calculate the moderated argument of the logit
    z = z_a * kappa_sig2_a

    # do a truncation to prevent exp overflow
    z = numpy.clip(z, -trunc, trunc)

    # get the moderated logistic probability
    pr = numpy.exp(z)
    pr = pr / (1. + pr)

    return pr

if __name__ == '__main__':
    file_dataset_word_frequency = '/Users/luotianyi/Desktop/CMPS242/project/'\
    'word_all_frequency_40000_20161105'
    file_dataset_train = '/Users/luotianyi/Desktop/CMPS242/project/'\
    'traindata_20161103'
    file_dataset_test = '/Users/luotianyi/Desktop/CMPS242/project/'\
    'testdata_20161103'
    word_diction = {} #word dictionary
    line_num = 0
    for line in open(file_dataset_word_frequency, 'r'):
        line_num += 1
        line_str_list = line.replace("\n", "").split("\t")
        word_diction[line_str_list[0]] = line_str_list[1]
    
    #construct the vector representation with the feature of naive bayes
    train_vectors = []
    test_vectors = []
    ylabels_train = []
    labels = []
    add_labels = []
    test_ylabels = []
    line_num = 0
    initial_theta = []
    #####################################
    #train tfidf feature construction
    #####################################
    for line in open(file_dataset_train, 'r'):
        line_num += 1
        if line_num % 5000 == 0:
            print "Triansets-" + str(line_num) + " lines are handled"
        #print "train_" + str(line_num)
        line_str_list = line.replace("\n", "").split("\t")
        temp_words_feature = []
        temp_text = line_str_list[1]
        ylabels_train.append(line_str_list[0])
        temp_words_list = temp_text.split(" ")
        for temp_word in word_diction:
            if line_num == 1:
                labels.append(temp_word)
                initial_theta.append(1.0)
            if str(temp_word) in temp_words_list:
                temp_words_feature.append(1.0 * (int(temp_words_list.count(str(temp_word)))))
            else:
                temp_words_feature.append(0.0)
        train_vectors.append(array(temp_words_feature))
    train_vectors_matrix = array(train_vectors)
    #print train_vectors_matrix[0]
    transformer = TfidfTransformer(smooth_idf=False)
    tfidf_train = transformer.fit_transform(train_vectors_matrix)
    train_tfidf_array = tfidf_train.toarray()
    train_ylabels_array = array(ylabels_train)
    print "Train tfidf feature construction finish!"
    #####################################
    #test tfidf feature construction
    #####################################
    line_num = 0
    for line in open(file_dataset_test, 'r'):
        line_num += 1
        if line_num % 5000 == 0:
            print "Testsets-" + str(line_num) + " lines are handled"
        #print "test_" + str(line_num)
        line_str_list = line.replace("\n", "").split("\t")
        temp_words_feature = []
        temp_text = line_str_list[1]
        test_ylabels.append(line_str_list[0])
        temp_words_list = temp_text.split(" ")
        for temp_word in word_diction:
            if line_num == 1:
                add_labels.append(temp_word)
            if str(temp_word) in temp_words_list:
                temp_words_feature.append(1.0 * (int(temp_words_list.count(str(temp_word)))))
            else:
                temp_words_feature.append(0.0)
        test_vectors.append(array(temp_words_feature))
    test_vectors_matrix = array(test_vectors)
    #print test_vectors_matrix[0]
    transformer = TfidfTransformer(smooth_idf=False)
    tfidf_test = transformer.fit_transform(test_vectors_matrix)
    test_tfidf_array = tfidf_test.toarray()
    test_ylabels_array = array(test_ylabels)
    print "Test tfidf feature construction finish!"
    print labels
    #print len(labels)
    
    #train the bayes logistic regression
    w_prior = numpy.zeros(len(word_diction))
    H_prior = numpy.diag(numpy.ones(len(word_diction)))*0.001
    w_posterior, H_posterior = fit_bayes_logistic(train_ylabels_array, train_tfidf_array, w_prior, H_prior)

    print "Predicting..."
    predict_test_prob_array = bayes_logistic_prob(test_tfidf_array, w_posterior, H_posterior)
    predict_test_labels = []
    item_num = 0
    test_right_num = 0
    for item_test in test_tfidf_array:
        item_num += 1
        predict_test_prob = predict_test_prob_array[item_num - 1]
        if predict_test_prob >= 0.5 and test_ylabels_array[item_num - 1] == '1':
            test_right_num += 1
        if predict_test_prob < 0.5 and test_ylabels_array[item_num - 1] == '0':
            test_right_num += 1
        #print predict_test_labels
    print "Predicting is finished!"
    print "The classification accuracy of test dataset is(Logistic Regression):"
    print 1.0 * test_right_num / len(test_ylabels_array)
